{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04091bee-88ba-4bed-94e9-fde160b1aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9cd245d-68f4-47b2-891d-7662b005dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4693d35c-03af-43a7-a293-e705abb16add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aefe742e-3b38-4f02-bf77-3c7b6d8d1cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x77f365f43e30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"section\", \"text\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28f7454-bdd3-4b47-ab69-583c70531120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query, course):\n",
    "    boost = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f785e94-397d-4ee5-88b5-61120c4ab0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4627/4627 [00:14<00:00, 310.27it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_search(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa75865-c9ac-4960-aebc-df4d56b782b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848714069591528"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ba9642-6c99-40bc-b7b1-867b95dda98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d2bf9b-1228-434a-b17e-513d417bf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf459430-cb84-4922-b035-be061938fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "536d4519-fc3b-4408-a83c-227cc6326953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x77f36567c860>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c1ae83-2c32-4c21-89b8-7c47b9668461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_vector(query, course):\n",
    "    boost = {'question': 1.0}\n",
    "\n",
    "    results = vindex.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84d23ee3-7613-4e5a-980f-19d1c792ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_vector(query, course):\n",
    "    #boost = {'question': 1.0}\n",
    "    \n",
    "    # Transform the query into the same vector space\n",
    "    query_vec = pipeline.transform([query])\n",
    "    \n",
    "    results = vindex.search(\n",
    "        query_vec,\n",
    "        filter_dict={'course': course},\n",
    "        #boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e03ac-860a-4f2c-9353-0e4306b0904d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70f818-32b9-4f5d-b1bf-6d5c49723264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07c7c7ee-15f7-40c9-9db0-69a92bd9e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4627/4627 [00:06<00:00, 705.21it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_vector(query=q['question'],course=q['course'])\n",
    "    relevance = [d['question'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d37e63-757d-473e-a3db-3bdfe951e171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f7ba9c7-3134-4b64-8b24-5adb1f59ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696af2b-ae4f-47b6-ae4b-e60f51f93644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a4e9906-1ac9-4779-88ea-b20d35e58aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64840d86-3a08-40b8-9d39-507c0eaeab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4627/4627 [00:06<00:00, 674.58it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_vector(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dab5adf6-8c8f-4dd7-8aed-07328fe7bf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3571284489590088"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5d48c8d-0a04-4c6b-8dd7-00e31bd9b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_qA = [f\"{d['question']} {d['text']}\" for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9b21dbe-abaa-4bd2-aaa1-4e62100af89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_qA = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X_qA = pipeline_qA.fit_transform(texts_qA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9be89bbe-fc6f-48f7-a953-cf190803e05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x77f365670950>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-index with minsearch, keeping the order aligned with `documents`\n",
    "from minsearch import VectorSearch\n",
    "vindex_qA = VectorSearch(keyword_fields={'course'})\n",
    "vindex_qA.fit(X_qA, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3609c7c1-7499-4a97-846e-5bca7a1a773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector search function (query gets vectorized with the *qA* pipeline)\n",
    "def minsearch_vector_qA(query, course):\n",
    "    qvec = pipeline_qA.transform([query])\n",
    "    return vindex_qA.search(\n",
    "        qvec,\n",
    "        filter_dict={'course': course},\n",
    "        num_results=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "286f1e0f-74ce-4800-9769-34eda64efbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4627/4627 [00:07<00:00, 635.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build relevance matrix using ID-to-ID comparison\n",
    "relevance_total_qA = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = str(q['document'])\n",
    "    results = minsearch_vector_qA(q['question'], q['course'])\n",
    "    row = [str(d['id']) == doc_id for d in results]\n",
    "    relevance_total_qA.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3cbfef1-401b-4f59-8c5e-040a380117b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate (Q + A vectors): 0.8210503566025502\n"
     ]
    }
   ],
   "source": [
    "# Your hit rate function works as-is\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt += 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "hitrate_qA = hit_rate(relevance_total_qA)\n",
    "print(\"Hit rate (Q + A vectors):\", hitrate_qA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87e63780-b0e0-4b82-b92c-84c5a5b16915",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ed2c4db-9351-4629-8c38-79e85fbce813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: qdrant-client in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.15.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.7.1+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant-client) (1.74.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /home/codespace/.local/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant-client) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from qdrant-client) (2.11.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /home/codespace/.local/lib/python3.12/site-packages (from qdrant-client) (2.5.0)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [sentence-transformers] \u001b[32m2/3\u001b[0m [sentence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed safetensors-0.6.2 sentence-transformers-5.1.0 transformers-4.55.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5354bb8-fe28-476c-b6a5-f1d27eb7fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.7.1+cpu)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (183.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1+cpu\n",
      "    Uninstalling torch-2.7.1+cpu:\n",
      "      Successfully uninstalled torch-2.7.1+cpu\n",
      "Successfully installed torch-2.8.0+cpu\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9fa6e-96fa-401c-a033-733a9a7fd32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97481751-a041-41f3-8814-bf44dc030b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e0a16a-1955-45cb-9e23-d3df24daa049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783848e-6dbf-4b3d-8057-43da03e7dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
      "- configuration_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
      "- modeling_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "# Model + data\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "model = SentenceTransformer(model_handle, trust_remote_code=True)\n",
    "\n",
    "texts_qA = [f\"{d['question']} {d['text']}\" for d in documents]\n",
    "embs = model.encode(texts_qA, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e92817-88c3-4ea3-9cfd-5689a393fba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83be88e-b1c3-48b9-b01d-c9d6880a9888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15881f8e-6c5e-4311-866f-e0098f1b7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_62926/3349582023.py:18: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     17\u001b[39m COLL = \u001b[33m\"\u001b[39m\u001b[33mqa_qdrant_jina_small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m client.recreate_collection(\n\u001b[32m     19\u001b[39m     collection_name=COLL,\n\u001b[32m     20\u001b[39m     vectors_config=VectorParams(size=\u001b[32m512\u001b[39m, distance=Distance.COSINE),\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m points = [\n\u001b[32m     24\u001b[39m     PointStruct(\n\u001b[32m     25\u001b[39m         \u001b[38;5;28mid\u001b[39m=\u001b[38;5;28mstr\u001b[39m(doc[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     26\u001b[39m         vector=embs[i].tolist(),\n\u001b[32m     27\u001b[39m         payload={\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(doc[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[33m\"\u001b[39m\u001b[33mcourse\u001b[39m\u001b[33m\"\u001b[39m: doc[\u001b[33m\"\u001b[39m\u001b[33mcourse\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: doc[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: doc[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m     28\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdocuments\u001b[49m)\n\u001b[32m     30\u001b[39m ]\n\u001b[32m     31\u001b[39m client.upsert(collection_name=COLL, points=points)\n",
      "\u001b[31mNameError\u001b[39m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "# Imports (run this in the same session where you’ll use Qdrant)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
    "\n",
    "# Model (Q+A embeddings)\n",
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v2-small-en\", trust_remote_code=True)\n",
    "\n",
    "# Qdrant client — pick ONE of these:\n",
    "# If you have a Qdrant server running:\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# If you DON'T have a server, use embedded/in-memory:\n",
    "client = QdrantClient(\":memory:\")  # or client = QdrantClient(path=\":memory:\")\n",
    "\n",
    "# Create/recreate collection (512-dim for this model, cosine distance)\n",
    "COLL = \"qa_qdrant_jina_small\"\n",
    "client.recreate_collection(\n",
    "    collection_name=COLL,\n",
    "    vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=str(doc[\"id\"]),\n",
    "        vector=embs[i].tolist(),\n",
    "        payload={\"id\": str(doc[\"id\"]), \"course\": doc[\"course\"], \"question\": doc[\"question\"], \"text\": doc[\"text\"]},\n",
    "    )\n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "client.upsert(collection_name=COLL, points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93189f2c-8f89-4a40-a0ea-75593df700f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant collection (512-d cosine for this model)\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "COLL = \"qa_qdrant_jina_small\"\n",
    "client.recreate_collection(\n",
    "    collection_name=COLL,\n",
    "    vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=str(doc[\"id\"]),\n",
    "        vector=embs[i].tolist(),\n",
    "        payload={\"id\": str(doc[\"id\"]), \"course\": doc[\"course\"], \"question\": doc[\"question\"], \"text\": doc[\"text\"]},\n",
    "    )\n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "client.upsert(collection_name=COLL, points=points)\n",
    "\n",
    "def qdrant_search(question, course, limit=5):\n",
    "    qvec = model.encode([question], normalize_embeddings=True)[0]\n",
    "    flt = Filter(must=[FieldCondition(key=\"course\", match=MatchValue(value=course))])\n",
    "    return client.search(COLL, query_vector=qvec.tolist(), query_filter=flt, with_payload=True, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6c7e1-f36d-4a2a-a474-af0fdda8fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Sanity checks (expects you already have these) ---\n",
    "# - documents: list of dicts with keys: id, question, text, course\n",
    "# - ground_truth: list of dicts with keys: question, course, document (the id of the correct doc)\n",
    "assert isinstance(documents, list) and len(documents) > 0, \"Expected non-empty `documents` list.\"\n",
    "assert isinstance(ground_truth, list) and len(ground_truth) > 0, \"Expected non-empty `ground_truth` list.\"\n",
    "\n",
    "# --- Helper: normalize course values (avoids case/whitespace mismatches) ---\n",
    "def norm_course(x):\n",
    "    return (x or \"\").strip().lower()\n",
    "\n",
    "# Ensure every doc has a normalized course for filtering\n",
    "for d in documents:\n",
    "    d[\"course_norm\"] = norm_course(d.get(\"course\", \"\"))\n",
    "\n",
    "# --- Build texts: question + answer ---\n",
    "texts_qA = [f\"{d['question']} {d['text']}\" for d in documents]\n",
    "\n",
    "# --- Load embedding model ---\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "model = SentenceTransformer(model_handle, trust_remote_code=True)\n",
    "\n",
    "# --- Encode corpus (normalize for cosine similarity) ---\n",
    "embs = model.encode(texts_qA, normalize_embeddings=True)\n",
    "embs = np.asarray(embs)\n",
    "vector_size = embs.shape[1]  # should be 512 for this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f9ed4-3565-4a9d-9a73-2816265965a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Qdrant: use in-memory client (switch to host/port if you have a running server) ---\n",
    "client = QdrantClient(\":memory:\")  # or QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "COLL = \"qa_qdrant_jina_small\"\n",
    "client.recreate_collection(\n",
    "    collection_name=COLL,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Optional: payload index for faster filtering by course\n",
    "# client.create_payload_index(collection_name=COLL, field_name=\"course_norm\", field_schema=\"keyword\")\n",
    "\n",
    "# --- Upsert points (store your doc id as the point id; keep id types consistent) ---\n",
    "points = []\n",
    "for i, doc in enumerate(documents):\n",
    "    pid = str(doc[\"id\"])\n",
    "    payload = {\n",
    "        \"id\": pid,\n",
    "        \"question\": doc[\"question\"],\n",
    "        \"text\": doc[\"text\"],\n",
    "        \"course\": doc[\"course\"],\n",
    "        \"course_norm\": doc[\"course_norm\"],\n",
    "    }\n",
    "    points.append(PointStruct(id=pid, vector=embs[i].tolist(), payload=payload))\n",
    "\n",
    "client.upsert(collection_name=COLL, points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ace9a-6896-4ac9-a3dc-90efef64c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Search helper: encode query, filter by normalized course, limit=5 ---\n",
    "def qdrant_search(question, course, limit=5):\n",
    "    qvec = model.encode([question], normalize_embeddings=True)[0].tolist()\n",
    "    flt = Filter(must=[FieldCondition(key=\"course_norm\", match=MatchValue(value=norm_course(course)))])\n",
    "    hits = client.search(\n",
    "        collection_name=COLL,\n",
    "        query_vector=qvec,\n",
    "        query_filter=flt,\n",
    "        with_payload=True,\n",
    "        limit=limit,\n",
    "    )\n",
    "    return hits  # list of ScoredPoint (.id, .score, .payload)\n",
    "\n",
    "# --- Metric: Mean Reciprocal Rank (MRR) for top-5 ---\n",
    "def mrr_qdrant(ground_truth, limit=5):\n",
    "    total = 0.0\n",
    "    for q in tqdm(ground_truth):\n",
    "        gold_id = str(q[\"document\"])\n",
    "        hits = qdrant_search(q[\"question\"], q[\"course\"], limit=limit)\n",
    "        rank = next((i + 1 for i, h in enumerate(hits) if str(h.id) == gold_id), None)\n",
    "        total += 0.0 if rank is None else 1.0 / rank\n",
    "    return total / len(ground_truth)\n",
    "\n",
    "# --- Compute & print MRR (limit=5 as requested) ---\n",
    "LIMIT = 5\n",
    "mrr_value = mrr_qdrant(ground_truth, limit=LIMIT)\n",
    "print(f\"MRR (Qdrant + {model_handle}, question+answer, top-{LIMIT}): {mrr_value:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
